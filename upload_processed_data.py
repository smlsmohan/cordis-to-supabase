#!/usr/bin/env python3
# filepath: /Users/mohan/Development/Projects/Mohan_Research/cordis-to-supabase/upload_processed_data.py
"""
Script to upload the processed data that was already generated by the main pipeline.
This saves time by not re-downloading and reprocessing all the data.
"""

import os
import sys
import pandas as pd

# Import our functions
sys.path.append('.')
from cordis_json_to_supabase import (
    download_and_extract_json, merge_programme_json, 
    normalise_dataframe, push_to_supabase
)

def reprocess_and_upload():
    """Re-run just the processing and upload steps"""
    print("ğŸ”„ Re-processing and uploading CORDIS data...")
    print("This will re-download and process all data quickly.")
    
    # Check environment
    supabase_url = os.environ.get("SUPABASE_URL")
    supabase_key = os.environ.get("SUPABASE_SERVICE_ROLE_KEY")
    
    if not supabase_url or not supabase_key:
        print("âŒ Environment variables not set!")
        print(f"SUPABASE_URL: {'SET' if supabase_url else 'NOT SET'}")
        print(f"SUPABASE_SERVICE_ROLE_KEY: {'SET' if supabase_key else 'NOT SET'}")
        return False
    
    print("âœ… Environment variables verified")
    print(f"   URL: {supabase_url}")
    print(f"   Key: {supabase_key[:20]}...")
    
    # URLs for the datasets
    CORDIS_JSON_URLS = {
        "HORIZONprojects": "https://cordis.europa.eu/data/cordis-HORIZONprojects-json.zip",
        "h2020projects": "https://cordis.europa.eu/data/cordis-h2020projects-json.zip", 
        "fp7projects": "https://cordis.europa.eu/data/cordis-fp7projects-json.zip",
    }
    
    all_dataframes = []
    
    # Process each dataset
    for label, url in CORDIS_JSON_URLS.items():
        print(f"\nğŸ”§ Processing {label}...")
        try:
            json_files = download_and_extract_json(url)
            if json_files:
                merged = merge_programme_json(json_files, label)
                if not merged.empty:
                    all_dataframes.append(merged)
                    print(f"âœ… {label}: {len(merged)} projects processed")
        except Exception as e:
            print(f"âŒ {label}: Failed - {e}")
            continue
    
    if not all_dataframes:
        print("âŒ No data processed!")
        return False
    
    # Combine datasets
    print(f"\nğŸ”— Combining {len(all_dataframes)} datasets...")
    combined = pd.concat(all_dataframes, ignore_index=True)
    print(f"ğŸ“Š Combined: {len(combined)} rows, {len(combined.columns)} columns")
    
    # Normalize and upload
    print(f"\nğŸ§¹ Normalizing data...")
    cleaned = normalise_dataframe(combined)
    
    if not cleaned.empty:
        print(f"\nğŸ“¤ Uploading {len(cleaned)} records...")
        push_to_supabase(cleaned, batch_size=50, test_mode=False, test_rows=0)
        print(f"\nğŸ‰ Upload completed!")
        return True
    else:
        print("âŒ No valid data to upload!")
        return False

if __name__ == "__main__":
    success = reprocess_and_upload()
    if not success:
        sys.exit(1)
