# CORDIS JSON to Supabase ETL Pipeline

This project downloads CORDIS (Community Research and Development Information Service) project data from JSON sources and uploads it to a Supabase database. **JSON sources are preferred over CSV for better data accuracy.**

## ğŸ†• **New JSON-Based Approach**

### **Data Sources (Priority Order):**
1. **ğŸ¥‡ Horizon Europe (PRIMARY)** - `https://cordis.europa.eu/data/cordis-HORIZONprojects-json.zip`
2. **ğŸ¥ˆ Horizon 2020** - `https://cordis.europa.eu/data/cordis-h2020projects-json.zip`  
3. **ğŸ¥‰ FP7** - `https://cordis.europa.eu/data/cordis-fp7projects-json.zip`

### **Key Features:**
- âœ… **Smart Data Merging**: Combines multiple JSON files within each dataset
- âœ… **Minimal Nulls**: Intelligent field population to reduce null values
- âœ… **Priority Processing**: Horizon Europe data is primary, others supplement
- âœ… **Robust Error Handling**: Continues processing even if some batches fail
- âœ… **Text Cleaning**: Handles special characters and large text fields

## ğŸš€ Quick Start

### **Prerequisites Complete âœ…**
- Python environment configured
- Dependencies installed  
- Supabase connection tested
- Database table created

### **Run the JSON Pipeline**

```bash
cd /Users/mohan/Development/Projects/Mohan_Research/cordis-to-supabase
./run_json.sh
```

### **Test First (Recommended)**

```bash
# Test JSON processing
/Users/mohan/Development/Projects/Mohan_Research/cordis-to-supabase/.venv/bin/python test_json.py

# Verify schema
/Users/mohan/Development/Projects/Mohan_Research/cordis-to-supabase/.venv/bin/python verify_schema.py
```

## ğŸ“Š **Data Processing Strategy**

### **1. Priority-Based Processing**
- **Horizon Europe**: All records included (primary dataset)
- **Horizon 2020**: Only new records not in Horizon Europe
- **FP7**: Only new records not in previous datasets

### **2. Intelligent Field Merging**
- Organizations â†’ `org_names`, `coordinator_names`, `org_countries`
- Topics â†’ `topics_codes`, `topics_desc`
- EuroSciVoc â†’ `euroSciVoc_labels`, `euroSciVoc_codes`
- Web Links â†’ `project_urls`

### **3. Data Quality Improvements**
- Text field cleaning and truncation
- Date standardization (YYYY-MM-DD)
- Numeric field validation
- Null value minimization

## ğŸ“ **Files Overview**

### **JSON Pipeline (New)**
- `cordis_json_to_supabase.py` - Main JSON ETL script â­
- `run_json.sh` - JSON pipeline runner â­
- `test_json.py` - JSON processing test

### **Legacy CSV Pipeline**
- `cordis_to_supabase.py` - Original CSV ETL script
- `run.sh` - CSV pipeline runner

### **Utilities**
- `verify_schema.py` - Schema verification
- `test_connection.py` - Connection test
- `fixed_schema.sql` - Database schema

## ğŸ”§ **Configuration**

Your setup is already configured:
- **Supabase URL**: `https://bfbhbaipgbazdhghrjho.supabase.co`
- **Service Key**: âœ… Configured
- **Table**: `cordis_projects`

## ğŸ“ˆ **Expected Results**

The JSON pipeline will:
1. **Download** ~500MB of JSON data from EU servers
2. **Process** 70,000+ project records across all programmes
3. **Merge** data intelligently to minimize null values
4. **Upload** in batches of 50 records to avoid timeouts
5. **Provide** detailed progress and error reporting

## âš¡ **Performance Improvements**

- **Smaller batches** (50 vs 100) for better reliability
- **Enhanced error recovery** - continues on failed batches
- **Text field optimization** - handles large descriptions
- **Memory efficient** - processes datasets sequentially

## ğŸ¯ **Why JSON is Better**

| Aspect | CSV | JSON âœ… |
|--------|-----|---------|
| **Data Structure** | Flat, limited | Nested, rich |
| **Data Types** | All strings | Native types |
| **Relationships** | Separate files | Embedded |
| **Encoding** | Semicolon issues | UTF-8 native |
| **Parsing** | Complex joins | Direct mapping |

## ğŸš€ **Ready to Run!**

```bash
# Run the complete JSON pipeline
./run_json.sh
```

**Expected runtime**: 15-30 minutes depending on network speed.

---

## ğŸ†˜ **Troubleshooting**

If you encounter issues:

1. **Test connectivity**: `python test_connection.py`
2. **Verify schema**: `python verify_schema.py`  
3. **Test JSON processing**: `python test_json.py`
4. **Check logs**: Look for specific error messages in terminal output

**Questions?** The pipeline provides detailed progress and error reporting! ğŸ‰
